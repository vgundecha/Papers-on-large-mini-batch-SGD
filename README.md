# Papers-on-large-mini-batch-SGD
A list of papers on large-batch distributed training of deep learning models

- Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [[pdf]] (https://arxiv.org/abs/1706.02677)
  - Priya Goyal, Piotr Doll√°r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He
  
- Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes [[pdf]] (https://arxiv.org/pdf/1711.04325)
  - Takuya Akiba, Shuji Suzuki, Keisuke Fukuda
  
- 100-epoch ImageNet Training with AlexNet in 24 Minutes [[pdf]] (https://arxiv.org/pdf/1711.04325)
  - You, Y., Zhang, Z., Hsieh, C.J. and Demmel, J.
  
- Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train [[pdf]] (https://arxiv.org/abs/1711.04291)
  - Valeriu Codreanu, Damian Podareanu, Vikram Saletore
  
- Train longer, generalize better: closing the generalization gap in large batch training of neural networks [[pdf]] (https://arxiv.org/pdf/1705.08741)
  - Hoffer, E., Hubara, I. and Soudry, D.
  
- Gradient Diversity: a Key Ingredient for Scalable Distributed Learning [[pdf]] (https://arxiv.org/abs/1705.08741)
  - Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, Peter Bartlett
  
- On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima [[pdf]] (https://arxiv.org/abs/1609.04836)
  - Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang
