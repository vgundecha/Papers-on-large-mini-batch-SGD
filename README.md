# Papers on large mini-batch SGD

There has been a lot interesting work on training with large batches recently. This is a list of papers on large-batch distributed training of deep learning models.

- Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour [[pdf]](https://arxiv.org/abs/1706.02677)
  - Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He
  
- Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes [[pdf]](https://arxiv.org/pdf/1711.04325)
  - Takuya Akiba, Shuji Suzuki, Keisuke Fukuda
  
- 100-epoch ImageNet Training with AlexNet in 24 Minutes [[pdf]](https://arxiv.org/pdf/1711.04325)
  - You, Y., Zhang, Z., Hsieh, C.J. and Demmel, J.
  
- Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train [[pdf]](https://arxiv.org/abs/1711.04291)
  - Valeriu Codreanu, Damian Podareanu, Vikram Saletore
  
- Train longer, generalize better: closing the generalization gap in large batch training of neural networks [[pdf]](https://arxiv.org/pdf/1705.08741)
  - Hoffer, E., Hubara, I. and Soudry, D.
  
- Gradient Diversity: a Key Ingredient for Scalable Distributed Learning [[pdf]](https://arxiv.org/abs/1705.08741)
  - Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, Peter Bartlett
  
- On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima [[pdf]](https://arxiv.org/abs/1609.04836)
  - Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang

- Don’t Decay the Learning Rate, Increase the Batch Size [[pdf]](https://arxiv.org/abs/1711.00489)
  - Smith, S.L., Kindermans, P.J. and Le, Q.V.
  
- ImageNet Training in Minutes [[pdf]](https://arxiv.org/abs/1709.05011?context=cs)
  - Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, Kurt Keutzer

- Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training [[pdf]](https://arxiv.org/abs/1712.01887)
  - Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally
